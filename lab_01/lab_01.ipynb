{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Experiment Tracking with TensorBoard\n",
    "\n",
    "Experiment tracking is essential for reproducible ML research. Without systematic tracking, you lose the ability to reproduce successful models, compare approaches, and understand what hyperparameters led to specific results.\n",
    "\n",
    "## High-Level Workflow\n",
    "\n",
    "![Experiment Tracking Workflow](./images/image.png)\n",
    "\n",
    "The workflow consists of **three phases**:\n",
    "\n",
    "| Phase | What Happens |\n",
    "|-------|--------------|\n",
    "| **Phase 1: Setup** | Initialize `SummaryWriter` and configure where logs are saved |\n",
    "| **Phase 2: Training** | Run training loop, collect metrics each epoch, save to event files |\n",
    "| **Phase 3: Analysis** | Launch TensorBoard, compare experiments, select best configuration |\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we'll learn to:\n",
    "- Integrate TensorBoard with PyTorch using `SummaryWriter`\n",
    "- Log metrics, hyperparameters, and model graphs\n",
    "- Organize and compare multiple experiments\n",
    "- Use TensorBoard UI for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision tensorboard tqdm requests -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poridhian/code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: First Experiment ‚Äî Single Run Tracking\n",
    "\n",
    "In this section, we'll run one complete experiment with TensorBoard logging. This covers **Phase 1 (Setup)** and **Phase 2 (Training)** from our workflow diagram.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Download a food image dataset (pizza, steak, sushi)\n",
    "2. Set up a pre-trained neural network (EfficientNet-B0)\n",
    "3. Train it while logging metrics to TensorBoard\n",
    "4. Visualize the results\n",
    "\n",
    "### Step 1: Download and Prepare Data\n",
    "\n",
    "We'll use a small food classification dataset. The images are organized in folders by class name, which PyTorch's `ImageFolder` can automatically load.\n",
    "\n",
    "![Download Dataset](./images/image7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Data downloaded!\n",
      "Train directory: data/pizza_steak_sushi/train\n",
      "Test directory: data/pizza_steak_sushi/test\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if not image_path.exists():\n",
    "    print(\"Downloading dataset...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        response = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(image_path)\n",
    "    print(\"Data downloaded!\")\n",
    "else:\n",
    "    print(\"Data already exists.\")\n",
    "\n",
    "# Setup paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['pizza', 'steak', 'sushi']\n",
      "Train samples: 225\n",
      "Test samples: 75\n"
     ]
    }
   ],
   "source": [
    "# Get transforms from pretrained model\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "auto_transforms = weights.transforms()\n",
    "\n",
    "# Create datasets\n",
    "train_data = datasets.ImageFolder(train_dir, transform=auto_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=auto_transforms)\n",
    "\n",
    "print(f\"Classes: {train_data.classes}\")\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Helper Functions\n",
    "\n",
    "Before training, we need two helper functions:\n",
    "- **`train_step()`** ‚Äî Runs one epoch of training (forward pass ‚Üí loss ‚Üí backward pass ‚Üí update weights)\n",
    "- **`test_step()`** ‚Äî Evaluates the model on test data (no weight updates)\n",
    "\n",
    "These functions will be called repeatedly during training, and we'll log their outputs to TensorBoard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    \"\"\"Single training epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (y_pred.argmax(1) == y).sum().item() / len(y)\n",
    "    \n",
    "    return train_loss / len(dataloader), train_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Single evaluation epoch.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item() / len(y)\n",
    "    \n",
    "    return test_loss / len(dataloader), test_acc / len(dataloader)\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize SummaryWriter\n",
    "\n",
    "This is **Phase 1 (Setup)** from our workflow. The `SummaryWriter` is TensorBoard's logging class ‚Äî it creates event files that TensorBoard reads to display your metrics.\n",
    "\n",
    "**What happens when you create a writer:**\n",
    "- A `runs/` directory is created (if it doesn't exist)\n",
    "- A timestamped subdirectory is created for this experiment\n",
    "- All logged data will be saved there as event files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: runs/Jan12_10-04-28_509b23b82278476a\n"
     ]
    }
   ],
   "source": [
    "# Create writer ‚Äî logs will be saved to runs/ directory\n",
    "writer = SummaryWriter()\n",
    "print(f\"TensorBoard logs will be saved to: {writer.log_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Training Function with TensorBoard Logging\n",
    "\n",
    "This is where **Phase 2 (Training)** happens. The training loop calls our helper functions and logs metrics to TensorBoard at each epoch.\n",
    "\n",
    "**Key TensorBoard methods we'll use:**\n",
    "\n",
    "| Method | Purpose | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| `add_scalars(tag, dict, step)` | Log multiple values on one chart | Train vs test metrics |\n",
    "| `add_graph(model, input)` | Log model architecture | Once after training |\n",
    "| `close()` | Flush data to disk | Always at the end |\n",
    "\n",
    "**Why `add_scalars` instead of `add_scalar`?** Using `add_scalars` with a dictionary plots both training and test metrics on the same chart, making it easy to spot overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined.\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    \"\"\"Training loop with TensorBoard logging.\"\"\"\n",
    "    \n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | \"\n",
    "              f\"test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # === TensorBoard Logging ===\n",
    "        writer.add_scalars(\"Loss\", {\"train\": train_loss, \"test\": test_loss}, epoch)\n",
    "        writer.add_scalars(\"Accuracy\", {\"train\": train_acc, \"test\": test_acc}, epoch)\n",
    "\n",
    "    # Log model graph\n",
    "    writer.add_graph(model, torch.randn(32, 3, 224, 224).to(device))\n",
    "    writer.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Model and DataLoaders\n",
    "\n",
    "We'll use **transfer learning** with EfficientNet-B0 ‚Äî a state-of-the-art image classifier pre-trained on ImageNet (millions of images).\n",
    "\n",
    "**Why transfer learning?**\n",
    "- Training from scratch requires millions of images and days of computation\n",
    "- Pre-trained models already understand general image features (edges, textures, shapes)\n",
    "- We only need to train the final layer to recognize our specific classes\n",
    "- Much faster and better results with small datasets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/poridhian/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 73.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4,011,391\n",
      "Trainable parameters: 3,843\n"
     ]
    }
   ],
   "source": [
    "# Create model (using pretrained EfficientNet-B0)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "\n",
    "# Freeze base layers, modify classifier\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1280, len(train_data.classes))\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 8\n",
      "Test batches: 3\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run the Experiment\n",
    "\n",
    "![Training Output](./images/image6.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [00:25<01:41, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0434 | train_acc: 0.5156 | test_loss: 0.9459 | test_acc: 0.5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:48<01:12, 24.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.9208 | train_acc: 0.5859 | test_loss: 0.7898 | test_acc: 0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:14<00:49, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.7465 | train_acc: 0.7578 | test_loss: 0.7067 | test_acc: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:38<00:24, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.6671 | train_acc: 0.7773 | test_loss: 0.6605 | test_acc: 0.8040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:03<00:00, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.6133 | train_acc: 0.8008 | test_loss: 0.6125 | test_acc: 0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test accuracy: 0.7936\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {results['test_acc'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: View Results in TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you'll see in TensorBoard:**\n",
    "- **Scalars tab**: Loss and accuracy curves over epochs (look for decreasing loss, increasing accuracy)\n",
    "- **Graphs tab**: Interactive model architecture visualization\n",
    "\n",
    "Access the dashboard at `http://<PUBLIC_IP>:6006` (get your IP by running `curl -s ifconfig.me` in terminal)\n",
    "\n",
    "![TensorBoard UI](./images/image4.png)\n",
    "\n",
    "**Scalars Tab:**\n",
    "\n",
    "![Scalars](./images/image8.png)\n",
    "\n",
    "**Graphs Tab:**\n",
    "\n",
    "![Graphs](./images/image9.png)\n",
    "\n",
    "---\n",
    "## Part 3: Second Experiment ‚Äî Multiple Hyperparameter Configurations\n",
    "\n",
    "Now we enter **Phase 3 (Analysis)** ‚Äî but first, we need more experiments to compare!\n",
    "\n",
    "Training a single model is rarely enough. In practice, you'll want to try different hyperparameters. This section shows how to:\n",
    "1. Run multiple experiments systematically\n",
    "2. Organize logs so they're easy to compare\n",
    "3. Use TensorBoard to find the best configuration\n",
    "\n",
    "**The experiments we'll run:**\n",
    "\n",
    "| Experiment | Learning Rate | Batch Size |\n",
    "|------------|---------------|------------|\n",
    "| 1 | 0.001 | 32 |\n",
    "| 2 | 0.001 | 64 |\n",
    "| 3 | 0.01 | 32 |\n",
    "| 4 | 0.01 | 64 |\n",
    "\n",
    "### Step 1: Create Custom Writer Function\n",
    "\n",
    "The default `SummaryWriter()` puts all experiments in one folder. We need a helper function to organize experiments into a structured directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom writer function defined.\n"
     ]
    }
   ],
   "source": [
    "def create_writer(experiment_name: str, model_name: str, extra: str = None):\n",
    "    \"\"\"Creates SummaryWriter with organized directory structure.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    log_dir = os.path.join(\"runs\", model_name, timestamp, experiment_name)\n",
    "    if extra:\n",
    "        log_dir = os.path.join(log_dir, extra)\n",
    "    \n",
    "    print(f\"[INFO] Logging to: {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(\"Custom writer function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Model Factory Function\n",
    "\n",
    "Each experiment needs a **fresh model** to ensure fair comparison. If we reused a trained model, subsequent experiments would start with learned weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model factory function defined.\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_classes=3):\n",
    "    \"\"\"Creates a fresh EfficientNet-B0 model for each experiment.\"\"\"\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(1280, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Model factory function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Modified Training Function\n",
    "\n",
    "This version accepts a writer as parameter instead of using a global one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified training function defined.\n"
     ]
    }
   ],
   "source": [
    "def train_with_writer(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer):\n",
    "    \"\"\"Training loop with TensorBoard logging using provided writer.\"\"\"\n",
    "    \n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | \"\n",
    "              f\"test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # TensorBoard Logging\n",
    "        writer.add_scalars(\"Loss\", {\"train\": train_loss, \"test\": test_loss}, epoch)\n",
    "        writer.add_scalars(\"Accuracy\", {\"train\": train_acc, \"test\": test_acc}, epoch)\n",
    "\n",
    "    # Log model graph\n",
    "    writer.add_graph(model, torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Modified training function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Hyperparameter Combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments to run: 4\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to test\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "total_experiments = len(learning_rates) * len(batch_sizes)\n",
    "print(f\"Total experiments to run: {total_experiments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run All Experiments\n",
    "\n",
    "**Expected output for each experiment:**\n",
    "\n",
    "![Experiment 1](./images/exp-1.png)\n",
    "![Experiment 2](./images/exp-2.png)\n",
    "![Experiment 3](./images/exp-3.png)\n",
    "![Experiment 4](./images/exp-4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment 1/4: LR=0.001, Batch=32\n",
      "============================================================\n",
      "[INFO] Logging to: runs/EfficientNetB0/2026-01-12/lr0.001_bs32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [00:23<01:35, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0285 | train_acc: 0.4453 | test_loss: 0.8359 | test_acc: 0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:46<01:10, 23.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.8661 | train_acc: 0.6094 | test_loss: 0.6937 | test_acc: 0.8456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:09<00:45, 22.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.7452 | train_acc: 0.7383 | test_loss: 0.7060 | test_acc: 0.7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:34<00:23, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.6953 | train_acc: 0.7578 | test_loss: 0.6170 | test_acc: 0.8258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:57<00:00, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.6021 | train_acc: 0.9062 | test_loss: 0.6263 | test_acc: 0.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.7538\n",
      "\n",
      "============================================================\n",
      "Experiment 2/4: LR=0.001, Batch=64\n",
      "============================================================\n",
      "[INFO] Logging to: runs/EfficientNetB0/2026-01-12/lr0.001_bs64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [00:27<01:50, 27.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0623 | train_acc: 0.4061 | test_loss: 0.9568 | test_acc: 0.6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:55<01:24, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.9078 | train_acc: 0.6657 | test_loss: 0.8299 | test_acc: 0.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:23<00:55, 27.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.7972 | train_acc: 0.8146 | test_loss: 0.7180 | test_acc: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:49<00:27, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.7028 | train_acc: 0.8224 | test_loss: 0.6395 | test_acc: 0.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:17<00:00, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.6081 | train_acc: 0.8794 | test_loss: 0.5760 | test_acc: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.9155\n",
      "\n",
      "============================================================\n",
      "Experiment 3/4: LR=0.01, Batch=32\n",
      "============================================================\n",
      "[INFO] Logging to: runs/EfficientNetB0/2026-01-12/lr0.01_bs32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [00:23<01:35, 23.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.1156 | train_acc: 0.4844 | test_loss: 0.4820 | test_acc: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:47<01:10, 23.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.4143 | train_acc: 0.8477 | test_loss: 0.3036 | test_acc: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:10<00:47, 23.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.4251 | train_acc: 0.7891 | test_loss: 0.4415 | test_acc: 0.8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:32<00:23, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.5830 | train_acc: 0.7617 | test_loss: 0.3939 | test_acc: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:56<00:00, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.4588 | train_acc: 0.7500 | test_loss: 0.4727 | test_acc: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8144\n",
      "\n",
      "============================================================\n",
      "Experiment 4/4: LR=0.01, Batch=64\n",
      "============================================================\n",
      "[INFO] Logging to: runs/EfficientNetB0/2026-01-12/lr0.01_bs64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [00:28<01:55, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.8854 | train_acc: 0.5756 | test_loss: 0.5529 | test_acc: 0.7166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:57<01:25, 28.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.4177 | train_acc: 0.8562 | test_loss: 0.3174 | test_acc: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:25<00:56, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.2065 | train_acc: 0.9111 | test_loss: 0.3415 | test_acc: 0.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:52<00:28, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.1797 | train_acc: 0.9180 | test_loss: 0.3506 | test_acc: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:21<00:00, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.0971 | train_acc: 0.9846 | test_loss: 0.3413 | test_acc: 0.8388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8388\n",
      "\n",
      "============================================================\n",
      "All experiments completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "experiment_num = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        experiment_num += 1\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment {experiment_num}/{total_experiments}: LR={lr}, Batch={bs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create experiment-specific writer\n",
    "        experiment_name = f\"lr{lr}_bs{bs}\"\n",
    "        writer = create_writer(experiment_name, \"EfficientNetB0\")\n",
    "        \n",
    "        # Create DataLoaders with current batch size\n",
    "        train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=bs, shuffle=False)\n",
    "        \n",
    "        # Fresh model for each experiment\n",
    "        model = create_model(num_classes=len(train_data.classes)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train with logging\n",
    "        results = train_with_writer(\n",
    "            model=model,\n",
    "            train_dataloader=train_loader,\n",
    "            test_dataloader=test_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            epochs=5,\n",
    "            device=device,\n",
    "            writer=writer\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[experiment_name] = results\n",
    "        \n",
    "        # Close writer before next experiment\n",
    "        writer.close()\n",
    "        \n",
    "        print(f\"Final test accuracy: {results['test_acc'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT COMPARISON\n",
      "============================================================\n",
      "Experiment           Final Test Acc  Best Test Acc  \n",
      "------------------------------------------------------------\n",
      "lr0.001_bs32         0.7538          0.8456         \n",
      "lr0.001_bs64         0.9155          0.9233         \n",
      "lr0.01_bs32          0.8144          0.8759         \n",
      "lr0.01_bs64          0.8388          0.9311         \n",
      "============================================================\n",
      "\n",
      "üèÜ Best experiment: lr0.001_bs64 with 0.9155 accuracy\n"
     ]
    }
   ],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Experiment':<20} {'Final Test Acc':<15} {'Best Test Acc':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "best_exp = None\n",
    "best_acc = 0\n",
    "\n",
    "for exp_name, results in all_results.items():\n",
    "    final_acc = results['test_acc'][-1]\n",
    "    best_acc_exp = max(results['test_acc'])\n",
    "    print(f\"{exp_name:<20} {final_acc:<15.4f} {best_acc_exp:<15.4f}\")\n",
    "    \n",
    "    if final_acc > best_acc:\n",
    "        best_acc = final_acc\n",
    "        best_exp = exp_name\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüèÜ Best experiment: {best_exp} with {best_acc:.4f} accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: View All Experiments in TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and launch TensorBoard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Final Comparison](./images/final.png)\n",
    "\n",
    "**TensorBoard comparison features:**\n",
    "\n",
    "| Feature | How to Use | Purpose |\n",
    "|---------|------------|---------|\n",
    "| **Run Selector** | Left sidebar checkboxes | Show/hide specific experiments |\n",
    "| **Regex Filter** | Type `lr0.001` in search | Filter to matching experiments |\n",
    "| **Smoothing Slider** | Adjust to reduce noise | See cleaner trends |\n",
    "\n",
    "**What to look for in comparisons:**\n",
    "- Which learning rate converges faster?\n",
    "- Which batch size gives better final accuracy?\n",
    "- Are any configurations unstable (high variance)?\n",
    "\n",
    "---\n",
    "## Part 4: Summary\n",
    "\n",
    "### Key TensorBoard Methods\n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|---------|\n",
    "| `add_scalar(tag, value, step)` | Log single metric | Learning rate |\n",
    "| `add_scalars(tag, dict, step)` | Log multiple metrics on one chart | Train vs test loss |\n",
    "| `add_graph(model, input)` | Log model architecture | Network visualization |\n",
    "| `add_histogram(tag, values, step)` | Log weight distributions | Debugging gradients |\n",
    "| `add_image(tag, img, step)` | Log images | Sample predictions |\n",
    "| `close()` | Flush and close writer | End of training |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Naming**: Use descriptive experiment names with hyperparameters (`lr0.001_bs32`)\n",
    "2. **Organization**: Structure directories by model/date/experiment\n",
    "3. **Consistency**: Log the same metrics across all experiments for comparison\n",
    "4. **Close writers**: Always call `writer.close()` to flush data to disk\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "Looking back at our workflow diagram:\n",
    "- ‚úÖ **Phase 1 (Setup)**: Initialized `SummaryWriter`, configured log directories\n",
    "- ‚úÖ **Phase 2 (Training)**: Logged metrics during training, saved event files\n",
    "- ‚úÖ **Phase 3 (Analysis)**: Launched TensorBoard, compared experiments, identified best config\n",
    "\n",
    "---\n",
    "## Next Steps\n",
    "\n",
    "In **Lab 2**, we'll explore data scaling ‚Äî training the same model on different dataset sizes to understand the data-performance relationship.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
