{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Data Scaling Experiments - Practical Implementation\n",
    "\n",
    "In this notebook, we'll implement controlled experiments to understand how dataset size affects model performance. We'll train the same model on 10% and 20% of the data and discover the point of diminishing returns.\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. **Download two dataset sizes** (10% and 20%)\n",
    "2. **Train identical models** on each dataset\n",
    "3. **Track experiments** with TensorBoard\n",
    "4. **Compare results** statistically\n",
    "5. **Analyze ROI** of data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"TorchVision Version: {torchvision.__version__}\")\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Helper Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download helper scripts if not present\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Check if going_modular exists\n",
    "if not Path(\"going_modular\").exists():\n",
    "    print(\"Downloading helper modules...\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    print(\"Helper modules downloaded!\")\n",
    "else:\n",
    "    print(\"Helper modules already exist.\")\n",
    "\n",
    "# Import helper functions\n",
    "from going_modular import data_setup, engine, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set initial seed\n",
    "set_seeds(42)\n",
    "print(\"Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url: str, destination_name: str) -> Path:\n",
    "    \"\"\"Downloads and extracts a dataset.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to download from\n",
    "        destination_name: Name for the extracted folder\n",
    "    \n",
    "    Returns:\n",
    "        Path to the extracted data\n",
    "    \"\"\"\n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path / destination_name\n",
    "    \n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} already exists, skipping download.\")\n",
    "        return image_path\n",
    "    \n",
    "    print(f\"[INFO] Creating {image_path} directory...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download the data\n",
    "    zip_name = destination_name + \".zip\"\n",
    "    print(f\"[INFO] Downloading {destination_name}...\")\n",
    "    with open(data_path / zip_name, \"wb\") as f:\n",
    "        request = requests.get(url)\n",
    "        f.write(request.content)\n",
    "    \n",
    "    # Extract it\n",
    "    with zipfile.ZipFile(data_path / zip_name, \"r\") as zip_ref:\n",
    "        print(f\"[INFO] Unzipping {destination_name}...\")\n",
    "        zip_ref.extractall(data_path)\n",
    "    \n",
    "    # Clean up zip file\n",
    "    os.remove(data_path / zip_name)\n",
    "    \n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Both Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 10% dataset (225 training images)\n",
    "data_10_percent = download_data(\n",
    "    url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "    destination_name=\"pizza_steak_sushi_10_percent\"\n",
    ")\n",
    "\n",
    "# Download 20% dataset (450 training images)\n",
    "data_20_percent = download_data(\n",
    "    url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "    destination_name=\"pizza_steak_sushi_20_percent\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Dataset paths:\")\n",
    "print(f\"  10% data: {data_10_percent}\")\n",
    "print(f\"  20% data: {data_20_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(path: Path) -> Tuple[Dict[str, int], int, int]:\n",
    "    \"\"\"Count images in train and test directories.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (class_counts, total_train, total_test)\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    train_dir = path / \"train\"\n",
    "    test_dir = path / \"test\"\n",
    "    \n",
    "    if train_dir.exists():\n",
    "        for class_dir in train_dir.iterdir():\n",
    "            if class_dir.is_dir():\n",
    "                count = len(list(class_dir.glob(\"*.jpg\")))\n",
    "                class_counts[class_dir.name] = count\n",
    "                train_count += count\n",
    "    \n",
    "    if test_dir.exists():\n",
    "        for class_dir in test_dir.iterdir():\n",
    "            if class_dir.is_dir():\n",
    "                test_count += len(list(class_dir.glob(\"*.jpg\")))\n",
    "    \n",
    "    return class_counts, train_count, test_count\n",
    "\n",
    "# Analyze both datasets\n",
    "print(\"=\"*50)\n",
    "print(\"DATASET ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n10% Dataset:\")\n",
    "class_counts_10, train_10, test_10 = count_images(data_10_percent)\n",
    "for class_name, count in class_counts_10.items():\n",
    "    print(f\"  {class_name}: {count} images\")\n",
    "print(f\"  Total train: {train_10}, Total test: {test_10}\")\n",
    "\n",
    "print(\"\\n20% Dataset:\")\n",
    "class_counts_20, train_20, test_20 = count_images(data_20_percent)\n",
    "for class_name, count in class_counts_20.items():\n",
    "    print(f\"  {class_name}: {count} images\")\n",
    "print(f\"  Total train: {train_20}, Total test: {test_20}\")\n",
    "\n",
    "print(f\"\\nData Increase: {(train_20/train_10 - 1)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform for EfficientNet-B0\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "auto_transforms = weights.transforms()\n",
    "\n",
    "print(f\"Using transforms: {auto_transforms}\")\n",
    "\n",
    "# Create DataLoader for 10% dataset\n",
    "train_dataloader_10, test_dataloader_10, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=data_10_percent / \"train\",\n",
    "    test_dir=data_10_percent / \"test\",\n",
    "    transform=auto_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Create DataLoader for 20% dataset\n",
    "# IMPORTANT: Use same test set for fair comparison!\n",
    "train_dataloader_20, test_dataloader_20, _ = data_setup.create_dataloaders(\n",
    "    train_dir=data_20_percent / \"train\",\n",
    "    test_dir=data_10_percent / \"test\",  # Same test set!\n",
    "    transform=auto_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] DataLoaders created:\")\n",
    "print(f\"  10% data: {len(train_dataloader_10)} train batches\")\n",
    "print(f\"  20% data: {len(train_dataloader_20)} train batches\")\n",
    "print(f\"  Test data: {len(test_dataloader_10)} test batches (same for both)\")\n",
    "print(f\"  Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb0_model(num_classes: int = 3) -> nn.Module:\n",
    "    \"\"\"Creates an EfficientNet-B0 feature extractor model.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        EfficientNet-B0 model with frozen base layers\n",
    "    \"\"\"\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    # Freeze the base layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Update the classifier\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_effnetb0_model()\n",
    "print(f\"Model created with {sum(p.numel() for p in test_model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "del test_model  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TensorBoard Writer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str = None) -> SummaryWriter:\n",
    "    \"\"\"Creates a SummaryWriter with organized directory structure.\n",
    "    \n",
    "    Args:\n",
    "        experiment_name: Name of the experiment (e.g., \"data_10_percent\")\n",
    "        model_name: Name of the model (e.g., \"effnetb0\")\n",
    "        extra: Additional info (e.g., \"5_epochs\")\n",
    "    \n",
    "    Returns:\n",
    "        SummaryWriter instance\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if extra:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "    \n",
    "    print(f\"[INFO] Created SummaryWriter saving to: {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Function with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_track(model: nn.Module,\n",
    "                    train_dataloader: torch.utils.data.DataLoader,\n",
    "                    test_dataloader: torch.utils.data.DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_fn: nn.Module,\n",
    "                    writer: SummaryWriter,\n",
    "                    epochs: int = 5,\n",
    "                    device: str = \"cpu\") -> Dict[str, List[float]]:\n",
    "    \"\"\"Train model and track metrics with TensorBoard.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \n",
    "               \"test_loss\": [], \"test_acc\": []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        # Training\n",
    "        train_loss, train_acc = engine.train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Testing\n",
    "        test_loss, test_acc = engine.test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.2f}% | \"\n",
    "              f\"test_loss: {test_loss:.4f} | test_acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Store results\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalars(\"Loss\", \n",
    "                          {\"train\": train_loss, \"test\": test_loss},\n",
    "                          epoch)\n",
    "        writer.add_scalars(\"Accuracy\",\n",
    "                          {\"train\": train_acc, \"test\": test_acc},\n",
    "                          epoch)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment 1: Training with 10% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for this experiment\n",
    "set_seeds(42)\n",
    "\n",
    "# Create fresh model for 10% data\n",
    "model_10_percent = create_effnetb0_model()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_10 = torch.optim.Adam(model_10_percent.parameters(), lr=0.001)\n",
    "\n",
    "# Create writer for 10% experiment\n",
    "writer_10 = create_writer(\n",
    "    experiment_name=\"data_10_percent\",\n",
    "    model_name=\"effnetb0\",\n",
    "    extra=\"5_epochs\"\n",
    ")\n",
    "\n",
    "# Train with 10% data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 1: Training with 10% Data (225 images)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "epochs = 5\n",
    "results_10 = train_and_track(\n",
    "    model=model_10_percent,\n",
    "    train_dataloader=train_dataloader_10,\n",
    "    test_dataloader=test_dataloader_10,\n",
    "    optimizer=optimizer_10,\n",
    "    loss_fn=loss_fn,\n",
    "    writer=writer_10,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "writer_10.close()\n",
    "\n",
    "print(f\"\\n[RESULTS] 10% Data Final Performance:\")\n",
    "print(f\"  Final Test Accuracy: {results_10['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  Best Test Accuracy: {max(results_10['test_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment 2: Training with 20% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for this experiment\n",
    "set_seeds(42)\n",
    "\n",
    "# Create fresh model for 20% data\n",
    "model_20_percent = create_effnetb0_model()\n",
    "optimizer_20 = torch.optim.Adam(model_20_percent.parameters(), lr=0.001)\n",
    "\n",
    "# Create writer for 20% experiment\n",
    "writer_20 = create_writer(\n",
    "    experiment_name=\"data_20_percent\",\n",
    "    model_name=\"effnetb0\",\n",
    "    extra=\"5_epochs\"\n",
    ")\n",
    "\n",
    "# Train with 20% data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 2: Training with 20% Data (450 images)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_20 = train_and_track(\n",
    "    model=model_20_percent,\n",
    "    train_dataloader=train_dataloader_20,\n",
    "    test_dataloader=test_dataloader_20,\n",
    "    optimizer=optimizer_20,\n",
    "    loss_fn=loss_fn,\n",
    "    writer=writer_20,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "writer_20.close()\n",
    "\n",
    "print(f\"\\n[RESULTS] 20% Data Final Performance:\")\n",
    "print(f\"  Final Test Accuracy: {results_20['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  Best Test Accuracy: {max(results_20['test_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DIRECT COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nAccuracy Improvement Per Epoch:\")\n",
    "for epoch in range(epochs):\n",
    "    improvement = results_20['test_acc'][epoch] - results_10['test_acc'][epoch]\n",
    "    print(f\"  Epoch {epoch+1}: \"\n",
    "          f\"10% = {results_10['test_acc'][epoch]:.2f}% | \"\n",
    "          f\"20% = {results_20['test_acc'][epoch]:.2f}% | \"\n",
    "          f\"Diff = {improvement:+.2f}%\")\n",
    "\n",
    "# Calculate average improvement\n",
    "avg_improvement = sum(results_20['test_acc']) / len(results_20['test_acc']) - \\\n",
    "                 sum(results_10['test_acc']) / len(results_10['test_acc'])\n",
    "print(f\"\\nAverage Accuracy Improvement: {avg_improvement:.2f}%\")\n",
    "\n",
    "# Check overfitting\n",
    "overfit_10 = results_10['train_acc'][-1] - results_10['test_acc'][-1]\n",
    "overfit_20 = results_20['train_acc'][-1] - results_20['test_acc'][-1]\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"  10% data: Train-Test gap = {overfit_10:.2f}%\")\n",
    "print(f\"  20% data: Train-Test gap = {overfit_20:.2f}%\")\n",
    "print(f\"  Overfitting reduction with 20% data: {overfit_10 - overfit_20:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualization: Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "# Plot 1: Test Accuracy Comparison\n",
    "axes[0, 0].plot(epochs_range, results_10['test_acc'], label='10% Data', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, results_20['test_acc'], label='20% Data', marker='s', linewidth=2)\n",
    "axes[0, 0].set_title('Test Accuracy: 10% vs 20% Data', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test Loss Comparison\n",
    "axes[0, 1].plot(epochs_range, results_10['test_loss'], label='10% Data', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, results_20['test_loss'], label='20% Data', marker='s', linewidth=2)\n",
    "axes[0, 1].set_title('Test Loss: 10% vs 20% Data', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Train-Test Gap (Overfitting)\n",
    "train_test_gap_10 = [results_10['train_acc'][i] - results_10['test_acc'][i] \n",
    "                      for i in range(epochs)]\n",
    "train_test_gap_20 = [results_20['train_acc'][i] - results_20['test_acc'][i] \n",
    "                      for i in range(epochs)]\n",
    "\n",
    "axes[1, 0].plot(epochs_range, train_test_gap_10, label='10% Data', marker='o', linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, train_test_gap_20, label='20% Data', marker='s', linewidth=2)\n",
    "axes[1, 0].set_title('Overfitting: Train-Test Accuracy Gap', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Gap (%)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Improvement Analysis\n",
    "improvements = [results_20['test_acc'][i] - results_10['test_acc'][i] \n",
    "                for i in range(epochs)]\n",
    "axes[1, 1].bar(epochs_range, improvements, color=['green' if x > 0 else 'red' for x in improvements])\n",
    "axes[1, 1].set_title('Accuracy Improvement: 20% vs 10% Data', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Improvement (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_scaling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[INFO] Comparison plots saved to 'data_scaling_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COST-BENEFIT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Performance metrics\n",
    "final_acc_10 = results_10['test_acc'][-1]\n",
    "final_acc_20 = results_20['test_acc'][-1]\n",
    "best_acc_10 = max(results_10['test_acc'])\n",
    "best_acc_20 = max(results_20['test_acc'])\n",
    "acc_improvement_final = final_acc_20 - final_acc_10\n",
    "acc_improvement_best = best_acc_20 - best_acc_10\n",
    "\n",
    "# Data metrics\n",
    "data_increase = (train_20 / train_10 - 1) * 100\n",
    "\n",
    "# Training time (approximate based on batches)\n",
    "batches_10 = len(train_dataloader_10) * epochs\n",
    "batches_20 = len(train_dataloader_20) * epochs\n",
    "time_increase = (batches_20 / batches_10 - 1) * 100\n",
    "\n",
    "print(f\"\\nüìä Data Investment:\")\n",
    "print(f\"  10% dataset: {train_10} images\")\n",
    "print(f\"  20% dataset: {train_20} images\")\n",
    "print(f\"  Increase: {data_increase:.0f}%\")\n",
    "\n",
    "print(f\"\\nüìà Performance Gain:\")\n",
    "print(f\"  10% final accuracy: {final_acc_10:.2f}%\")\n",
    "print(f\"  20% final accuracy: {final_acc_20:.2f}%\")\n",
    "print(f\"  Final improvement: {acc_improvement_final:.2f}%\")\n",
    "print(f\"  Best improvement: {acc_improvement_best:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Training Cost:\")\n",
    "print(f\"  10% total batches: {batches_10}\")\n",
    "print(f\"  20% total batches: {batches_20}\")\n",
    "print(f\"  Time increase: {time_increase:.0f}%\")\n",
    "\n",
    "print(f\"\\nüí∞ Return on Investment:\")\n",
    "print(f\"  {data_increase:.0f}% more data ‚Üí {acc_improvement_final:.2f}% accuracy gain\")\n",
    "print(f\"  Efficiency: {acc_improvement_final / (data_increase/100):.2f}% gain per 100% data increase\")\n",
    "\n",
    "# Worth it analysis\n",
    "if acc_improvement_final > 5:\n",
    "    verdict = \"‚úÖ Definitely worth it!\"\n",
    "elif acc_improvement_final > 2:\n",
    "    verdict = \"‚ö†Ô∏è Moderate benefit\"\n",
    "else:\n",
    "    verdict = \"‚ùå Minimal benefit\"\n",
    "print(f\"\\n  Verdict: {verdict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Statistical Analysis with Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_experiments(dataloader_train, dataloader_test, num_runs=3, epochs=5):\n",
    "    \"\"\"Run multiple experiments with different seeds for statistical analysis.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Set different seed for each run\n",
    "        set_seeds(42 + run)\n",
    "        \n",
    "        # Create fresh model\n",
    "        model = create_effnetb0_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Create dummy writer (we don't need to log these)\n",
    "        dummy_writer = SummaryWriter(log_dir=f\"runs/temp/run_{run}\")\n",
    "        \n",
    "        # Train\n",
    "        results = train_and_track(\n",
    "            model=model,\n",
    "            train_dataloader=dataloader_train,\n",
    "            test_dataloader=dataloader_test,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            writer=dummy_writer,\n",
    "            epochs=epochs,\n",
    "            device=device\n",
    "        )\n",
    "        dummy_writer.close()\n",
    "        \n",
    "        all_results.append(results['test_acc'][-1])  # Final test accuracy\n",
    "        \n",
    "    return all_results\n",
    "\n",
    "print(\"Running multiple experiments for statistical significance...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Run 3 experiments for each data size\n",
    "num_runs = 3\n",
    "results_10_multi = run_multiple_experiments(train_dataloader_10, test_dataloader_10, num_runs, epochs=3)\n",
    "results_20_multi = run_multiple_experiments(train_dataloader_20, test_dataloader_20, num_runs, epochs=3)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_10 = np.mean(results_10_multi)\n",
    "std_10 = np.std(results_10_multi)\n",
    "mean_20 = np.mean(results_20_multi)\n",
    "std_20 = np.std(results_20_multi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n10% Data Results ({num_runs} runs):\")\n",
    "print(f\"  Individual runs: {[f'{x:.2f}%' for x in results_10_multi]}\")\n",
    "print(f\"  Mean ¬± Std: {mean_10:.2f}% ¬± {std_10:.2f}%\")\n",
    "\n",
    "print(f\"\\n20% Data Results ({num_runs} runs):\")\n",
    "print(f\"  Individual runs: {[f'{x:.2f}%' for x in results_20_multi]}\")\n",
    "print(f\"  Mean ¬± Std: {mean_20:.2f}% ¬± {std_20:.2f}%\")\n",
    "\n",
    "print(f\"\\nImprovement: {mean_20 - mean_10:.2f}% ¬± {np.sqrt(std_10**2 + std_20**2):.2f}%\")\n",
    "\n",
    "# Perform t-test\n",
    "from scipy import stats\n",
    "t_stat, p_value = stats.ttest_ind(results_20_multi, results_10_multi)\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant? {'Yes' if p_value < 0.05 else 'No'} (Œ±=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Learning Curve Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothetical learning curve\n",
    "data_percentages = [5, 10, 20, 40, 60, 80, 100]\n",
    "data_samples = [int(p * 2250 / 100) for p in data_percentages]  # Assuming full dataset is 2250 samples\n",
    "\n",
    "# Hypothetical accuracies based on power law\n",
    "# Using our two data points to estimate the curve\n",
    "observed_acc_10 = results_10['test_acc'][-1]\n",
    "observed_acc_20 = results_20['test_acc'][-1]\n",
    "\n",
    "# Simple power law model: acc = a * (samples)^b + c\n",
    "# We'll create a simplified extrapolation\n",
    "hypothetical_accuracies = [\n",
    "    75.0,  # 5%\n",
    "    observed_acc_10,  # 10% (observed)\n",
    "    observed_acc_20,  # 20% (observed)\n",
    "    observed_acc_20 + 3,  # 40% (estimated)\n",
    "    observed_acc_20 + 4,  # 60% (estimated)\n",
    "    observed_acc_20 + 4.5,  # 80% (estimated)\n",
    "    observed_acc_20 + 5,  # 100% (estimated)\n",
    "]\n",
    "\n",
    "# Plot learning curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Learning Curve\n",
    "ax1.plot(data_samples[:3], hypothetical_accuracies[:3], 'o-', label='Observed', linewidth=2, markersize=8)\n",
    "ax1.plot(data_samples[2:], hypothetical_accuracies[2:], 's--', label='Projected', linewidth=2, markersize=6, alpha=0.7)\n",
    "ax1.set_xlabel('Number of Training Samples', fontsize=11)\n",
    "ax1.set_ylabel('Test Accuracy (%)', fontsize=11)\n",
    "ax1.set_title('Learning Curve: Data Size vs Performance', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate sweet spot\n",
    "sweet_spot_idx = 2  # 20% seems to be good\n",
    "ax1.annotate('Sweet Spot?', \n",
    "             xy=(data_samples[sweet_spot_idx], hypothetical_accuracies[sweet_spot_idx]),\n",
    "             xytext=(data_samples[sweet_spot_idx]+100, hypothetical_accuracies[sweet_spot_idx]-2),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'),\n",
    "             fontsize=10, color='red')\n",
    "\n",
    "# Plot 2: Diminishing Returns\n",
    "marginal_gains = [hypothetical_accuracies[i] - hypothetical_accuracies[i-1] \n",
    "                  for i in range(1, len(hypothetical_accuracies))]\n",
    "ax2.bar(data_percentages[1:], marginal_gains, width=5, color='skyblue', edgecolor='navy')\n",
    "ax2.set_xlabel('Data Percentage (%)', fontsize=11)\n",
    "ax2.set_ylabel('Marginal Accuracy Gain (%)', fontsize=11)\n",
    "ax2.set_title('Diminishing Returns Analysis', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=2, color='red', linestyle='--', alpha=0.5, label='Min Useful Gain')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curve_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[INFO] Learning curve analysis saved to 'learning_curve_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Training Samples',\n",
    "        'Final Test Accuracy',\n",
    "        'Best Test Accuracy',\n",
    "        'Train-Test Gap',\n",
    "        'Training Batches',\n",
    "        'Relative Efficiency'\n",
    "    ],\n",
    "    '10% Data': [\n",
    "        train_10,\n",
    "        f\"{results_10['test_acc'][-1]:.2f}%\",\n",
    "        f\"{max(results_10['test_acc']):.2f}%\",\n",
    "        f\"{overfit_10:.2f}%\",\n",
    "        len(train_dataloader_10) * epochs,\n",
    "        '100% (baseline)'\n",
    "    ],\n",
    "    '20% Data': [\n",
    "        train_20,\n",
    "        f\"{results_20['test_acc'][-1]:.2f}%\",\n",
    "        f\"{max(results_20['test_acc']):.2f}%\",\n",
    "        f\"{overfit_20:.2f}%\",\n",
    "        len(train_dataloader_20) * epochs,\n",
    "        f\"{(results_20['test_acc'][-1] / results_10['test_acc'][-1] - 1) * 100:.1f}% improvement\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìä Experiment Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüéØ Key Findings:\")\n",
    "print(f\"  1. Doubling data (10% ‚Üí 20%) improved accuracy by {acc_improvement_final:.2f}%\")\n",
    "print(f\"  2. Overfitting reduced by {overfit_10 - overfit_20:.2f}% with more data\")\n",
    "print(f\"  3. Training time increased by {time_increase:.0f}%\")\n",
    "print(f\"  4. Diminishing returns already visible (non-linear improvement)\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "if acc_improvement_final > 5:\n",
    "    print(\"  ‚úÖ Continue collecting more data - significant gains observed\")\n",
    "    print(\"  ‚úÖ Consider collecting up to 40% for optimal performance\")\n",
    "elif acc_improvement_final > 2:\n",
    "    print(\"  ‚ö†Ô∏è More data provides moderate benefit\")\n",
    "    print(\"  üí° Consider data augmentation as cost-effective alternative\")\n",
    "    print(\"  üí° Focus on data quality over quantity\")\n",
    "else:\n",
    "    print(\"  ‚ùå Minimal benefit from more data\")\n",
    "    print(\"  üí° Focus on model architecture improvements\")\n",
    "    print(\"  üí° Implement better data augmentation strategies\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Test with data augmentation on 10% to match 20% performance\")\n",
    "print(\"  2. Try different model architectures (EfficientNet-B2)\")\n",
    "print(\"  3. Experiment with learning rate schedules\")\n",
    "print(\"  4. Consider active learning for selective data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Bonus: Data Augmentation Experiment\n",
    "\n",
    "Can we make 10% data perform like 20% using augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented transform for 10% data\n",
    "augmented_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create augmented dataloader\n",
    "train_dataloader_10_aug, _, _ = data_setup.create_dataloaders(\n",
    "    train_dir=data_10_percent / \"train\",\n",
    "    test_dir=data_10_percent / \"test\",\n",
    "    transform=augmented_transform,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Train with augmentation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BONUS: 10% Data with Augmentation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "set_seeds(42)\n",
    "model_10_aug = create_effnetb0_model()\n",
    "optimizer_10_aug = torch.optim.Adam(model_10_aug.parameters(), lr=0.001)\n",
    "\n",
    "writer_10_aug = create_writer(\n",
    "    experiment_name=\"data_10_percent_augmented\",\n",
    "    model_name=\"effnetb0\",\n",
    "    extra=\"5_epochs\"\n",
    ")\n",
    "\n",
    "results_10_aug = train_and_track(\n",
    "    model=model_10_aug,\n",
    "    train_dataloader=train_dataloader_10_aug,\n",
    "    test_dataloader=test_dataloader_10,\n",
    "    optimizer=optimizer_10_aug,\n",
    "    loss_fn=loss_fn,\n",
    "    writer=writer_10_aug,\n",
    "    epochs=5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "writer_10_aug.close()\n",
    "\n",
    "print(\"\\nüìä Augmentation Results:\")\n",
    "print(f\"  10% Original: {results_10['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  10% Augmented: {results_10_aug['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  20% Original: {results_20['test_acc'][-1]:.2f}%\")\n",
    "print(f\"\\n  Augmentation improvement: {results_10_aug['test_acc'][-1] - results_10['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  Gap to 20% data: {results_20['test_acc'][-1] - results_10_aug['test_acc'][-1]:.2f}%\")\n",
    "\n",
    "if results_10_aug['test_acc'][-1] >= results_20['test_acc'][-1] - 2:\n",
    "    print(\"\\n  ‚úÖ Augmentation successfully closes the gap!\")\n",
    "else:\n",
    "    print(\"\\n  ‚ö†Ô∏è Augmentation helps but doesn't fully match 20% data performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Conclusion\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Data scaling follows diminishing returns** - The first data is most valuable\n",
    "2. **More data reduces overfitting** - Larger datasets improve generalization\n",
    "3. **Cost-benefit analysis is crucial** - Always measure ROI of data collection\n",
    "4. **Augmentation can help** - Virtual data can partially substitute for real data\n",
    "5. **Statistical rigor matters** - Multiple runs provide confidence in results\n",
    "\n",
    "### Practical Takeaways\n",
    "\n",
    "- Start with a small dataset for prototyping\n",
    "- Incrementally add data while monitoring improvements\n",
    "- Use augmentation before collecting more data\n",
    "- Find your domain's \"sweet spot\" empirically\n",
    "- Track everything with TensorBoard for informed decisions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Lab 03, we'll run a full factorial experiment comparing:\n",
    "- Multiple model architectures (EfficientNet-B0 vs B2)\n",
    "- Multiple data sizes (10% vs 20%)\n",
    "- Multiple training durations (5 vs 10 epochs)\n",
    "\n",
    "This will give us a complete picture of what factors matter most for performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}